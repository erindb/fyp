---
title: "Narrative Cloze Task: Experiment 3"
author: |
  | Erin Bennett
  | `erindb@stanford.edu`
bibliography: ../bibliography.bib
output:
  pdf_document:
    toc: false
    highlight: zenburn
    toc_depth: 3
---

```{r global_options, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE, sanitiz =TRUE, fig.width=5, fig.height=3)
```

```{r}
library(tidyr)
library(dplyr)
library(pwr)
library(rjson)
library(ggplot2)
library(ggthemes)
library(entropy)
library(bootstrap)
theme_new <- theme_set(theme_few())

# for bootstrapping 95% confidence intervals
theta <- function(x,xdata) {mean(xdata[x], na.rm=T)}
ci.low <- function(x) {
  quantile(bootstrap::bootstrap(1:length(x),1000,theta,x)$thetastar,.025)}
ci.high <- function(x) {
  quantile(bootstrap::bootstrap(1:length(x),1000,theta,x)$thetastar,.975)}

options(digits=3)
```

# Background

Scripts are a kind of knowledge that people have that helps them understand narrative texts, including filling in their representation of the story with events that were not mentioned but that probably happened.

Recent NLP work has developed initial script induction models and evaluated their performance at filling in the gaps in narrative text. Some of these models have access only to very pared-down linguistic information (essentially only the main verb of a sentence), which allows them to combine multiple sentences together but potentially looses a lot of important information, too.

# Question

How do people fill in gaps in narrative text?

* To what extent can people accurately fill in gaps in natural narrative text?
* Whether or not people are able to fill in the original text, do they tend to agree with one another?
* How detailed does the contextual linguistic information need to be for people to sensibly fill in the gaps in narrative texts?

And how does this relate to the performance of script induction models?

# Experiment

## Design

51 Stimuli:

* 17 documents randomly sampled from "Dinners from Hell" blog (within Ss)
* 3 event chain / cloze test pairs for each document (between Ss)

3 linguistic conditions (between Ss):

* caveman
* event only
* original text
    
### Pilot results

In the pilot, we lost data from some participants, and unfortunately, this was length/time based and so the most conscientious participants were excluded.

There were 51 cloze tasks in total, and only 17 were seen by each participant. We collected data from 24 participants, so not all of the cloze tasks had enough responses to make meaningful inferences about (e.g.) response overlap across participants.

```{r}
d = read.table('verb-processed-data.txt', sep=' ')
names(d) = c('doc', 'chain', 'cloze', 'cond', 'prop.match.orig',
             'prop.match.mode', 'n.match.mode', 'n', 'mode', 'loosely.orig', 'orig', 'n.loose.mode')
d = d %>% mutate(prop.loosely.match = loosely.orig/n,
                 cond = factor(cond, levels=c('event_only', 'caveman', 'original')),
                 tag = paste(doc, chain, cloze, cond))
```

#### Question 1: To what extent can people accurately fill in gaps in natural narrative text?

First, we wanted to know whether the main verb of the original sentence was mentioned in the participants' responses. We searched wordnet for synonyms of each of the verbs that participants used checked for any overlap with the synonyms of the original main verb.

```{r}
# how many distince cloze tasks were in original text condition?
n.cloze.orig = length(unique((d %>% filter(cond=='original'))$tag))

recovered.from.orig.text = d %>% filter(cond=='original' & prop.loosely.match>0)
```

Of the `r n.cloze.orig` stimuli presented with the original passage text as context, only `r nrow(recovered.from.orig.text)` of them had main verbs that were shown to be recovered (i.e. shared a synonym with one a response verb) by at least one participant.
<!--
 replied forget getting the manager... we are done here!
[We started on our way out of the restaurant.]
The manager was following us.
(length 9 events in total)

[We waited some more, but only until we could snag our waiter, who was now quite a bit busier, since nearly an hour had passed and many more people had come in.]
We got our waiter's attention and explained that we had not received our food, even though we ordered before the other party had even arrived.
We explained the situation and she called our waiter over.
(length 3 events in total)

```{r}
d %>% filter(cond=='original') %>%
  ggplot(aes(x=prop.loosely.match)) +
  geom_histogram(binwidth=0.01)
```
-->

```{r}
df = read.csv('experiment3-annotated.csv')

hand_annotated_stats = df %>% filter(condition=='original') %>%
  group_by(document, chain, clozeIndex) %>%
  summarise(prop.match = mean(vpmatch=='yes'),
            any.match = prop.match>0)

prop.tasks.with.hand.match = mean(hand_annotated_stats$any.match)
prop.people.give.success = mean((hand_annotated_stats %>% filter(any.match))$prop.match)
```

However, hand-coding whether the gist of the response was similar to the gist of the original event, we can see that people are actually doing a lot better on this task. Still, only `r prop.tasks.with.hand.match`% of the stimuli were recovered by any of the participants, and on those conditions, only `r prop.people.give.success`% of participants chose that response.

```{r}
hand_annotated_vs_synset_matching = rbind(
  d %>%
    rename(document=doc, clozeIndex=cloze, condition=cond, prop.match=prop.loosely.match) %>%
    select(document, chain, clozeIndex, prop.match, condition) %>%
    mutate(matching.method='auto'),
  df %>%
    group_by(document, chain, clozeIndex, condition) %>% 
    summarise(prop.match = mean(vpmatch=='yes')) %>%
    mutate(matching.method='manual'))

hand_annotated_vs_synset_matching %>% filter(condition=='original') %>%
  group_by(matching.method) %>%
  summarise(low = ci.low(prop.match),
            high= ci.high(prop.match),
            prop.match = mean(prop.match)) %>%
  ggplot(., aes(x=matching.method, y=prop.match)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(x=matching.method, ymin=low, ymax=high), width=0.01) +
  ylim(0, 1) +
  ylab("proportion of participants who recovered original text") +
  xlab("method for determining match") +
  ggtitle("participants' recovery of left-out original text")
ggsave('human.cloze.orig.png', width=8, height=5)
```

Overall, people don't seem to perform especially well on the task of recovering left-out sentences.

#### Question 2: Whether or not people are able to fill in the original text, do they tend to agree with one another?

We also wanted to know to what extent people agree on the main verb for the sentences they produce for a given cloze task.

We used Stanford CoreNLP dependency parser to extract the root verb (or occasionally adjectival predicate: e.g. 'red' in 'The apple is red') from the sentences that people produced. For each predicate, we looked for the first synonym set in wordnet. If there was a synonymset for that word, we replaced it with the name of that synset (e.g. we replaced "travel" with its synset name "go").

```{r, fig.width=4, fig.height=8}
#histogram
df = read.csv('experiment3-with-main-verbs.csv') %>%
  mutate(synset = as.character(synset),
         root.predicate = as.character(root.predicate),
         synset=factor(synset, levels=names(table(synset))[rev(order(table(synset)))]),
         tag = paste(document, chain, clozeIndex))

# # not all the synset names were the same as the response predicate
# df %>% filter(synset!=root.predicate) %>% select(synset, root.predicate)

# # overall, words are kinda ziphian
# df %>% ggplot(., aes(x=synset)) +
#   geom_bar(stat='count') +
#   theme(axis.text.x = element_blank())

df %>% filter(condition=='original') %>%
  ggplot(., aes(x=synset)) +
  geom_bar(stat='count') +
  facet_wrap(~tag, scale='free_x') +
  # theme(axis.text.x = element_blank())
  theme(axis.text.x = element_text(angle=60, hjust=1))
ggsave('agreement-hist-orig.png', width=4, height=8)
```

For most of the cloze tasks, there was not much overlap in responses.

In addition to this automated method of finding main event predicates, we also hand-annotated the pilot data with the experimenter's gloss of the main verb.

```{r, fig.width=4, fig.height=8}
#histogram
df %>% filter(condition=='original') %>%
  ggplot(., aes(x=vp)) +
  geom_bar(stat='count') +
  facet_wrap(~tag, scale='free_x') +
  # theme(axis.text.x = element_blank())
  theme(axis.text.x = element_text(angle=60, hjust=1))
ggsave('agreement-hist-orig-by-hand.png', width=4, height=8)
```

This method shows a tiny bit more overlap in participants' responses.

For all cloze tasks in the pilot where we have data from more than one participant, we calculated the entropy of the distribution of responses and divided by the maximum possible entropy given the number of participants. We subtracted this number from one to get an "agreement" measure. So a value of 0 corresponds to no agreement, and larger values correspond to more agreement. (Entropy by itself will not be an informative measure of agreement, since we have different numbers of participants in each condition.)

<!-- robert's python package: http://homes.soic.indiana.edu/sdedeo/page7/page7.html -->

```{r}
#entropy/agreement measure

agreement.df = df %>%
  group_by(tag, condition) %>%
  summarise(max.possible.entropy = log(length(tag)),
            auto.entropy = entropy(table(synset)),
            manual.entropy = entropy(table(vp))) %>%
  filter(max.possible.entropy > 0) %>%
  mutate(auto.agreement = 1 - (auto.entropy/max.possible.entropy),
         manual.agreement = 1 - (manual.entropy/max.possible.entropy)) %>%
  select(tag, auto.agreement, manual.agreement, condition) %>%
  gather(method, agreement, 2:3) %>%
  separate(method, into=c('method', 'trash')) %>% select(-trash)

agreement.df %>% filter(condition=='original') %>% group_by(method) %>%
  summarise(low = ci.low(agreement),
            high = ci.high(agreement),
            agreement = mean(agreement)) %>%
  ggplot(., aes(x=method, y=agreement)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(x=method, ymin=low, ymax=high), width=0.01) +
  ylim(0, 1)
```

Overall, people didn't agree with one another very much on their answers to these cloze tasks.

#### Question 3: How detailed does the contextual linguistic information need to be for people to sensibly fill in the gaps in narrative texts?

```{r}
# success by condition (faceted by method)

hand_annotated_vs_synset_matching %>%
  filter(condition != '') %>%
  group_by(matching.method, condition) %>%
  summarise(low = ci.low(prop.match),
            high= ci.high(prop.match),
            prop.match = mean(prop.match, na.rm=T)) %>%
  ggplot(., aes(x=condition, y=prop.match, fill=condition)) +
  geom_bar(stat='identity') +
  geom_errorbar(aes(x=condition, ymin=low, ymax=high), width=0.01) +
  facet_wrap(~matching.method) +
  ylim(0, 1)
```

When we manually checked responses for having similar meaning to the original sentences, we find that including the full original text from the context increases the probability that people will recover something close to the original left-out sentence.

```{r}
# agreement by condition (faceted by method)

agreement.df %>%
  filter(condition != '') %>%
  group_by(method, condition) %>%
  summarise(low = ci.low(agreement),
            high = ci.high(agreement),
            agreement = mean(agreement)) %>%
  ggplot(., aes(x=condition, y=agreement, fill=condition)) +
  geom_bar(stat='identity') +
  facet_wrap(~method) +
  geom_errorbar(aes(x=condition, ymin=low, ymax=high), width=0.01) +
  ylim(0, 1)
```

We found no difference in agreement across the three linguistic conditions.

### Question 4: How does human performance on this task relate to the performance of a PMI-based script induction model?

### Informativity in human responses

is there anything in the data that suggests people are being informative?

## Informativity model of narrative generation

## Future work

One important observation is that most things that are mentioned in a narrative are at least a little bit uncommon given the context, otherwise they would not be interesting/informative enough to mention. How does this affect what people and machines can learn from narrative text?

# References